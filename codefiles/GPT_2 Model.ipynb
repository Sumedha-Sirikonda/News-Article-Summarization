{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GPT_2 Model.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"0XdLpvOMnUF6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637456950487,"user_tz":300,"elapsed":13996,"user":{"displayName":"Bhanu Chander Reddy Sama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11248088089029933305"}},"outputId":"ba3b0916-47db-4e6b-da5e-81808c3ef833"},"source":["!pip install transformers\n","!pip install gensim"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 4.5 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 32.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 23.0 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 35.9 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UilclSei1zsr","executionInfo":{"status":"ok","timestamp":1637456969767,"user_tz":300,"elapsed":12363,"user":{"displayName":"Bhanu Chander Reddy Sama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11248088089029933305"}},"outputId":"ec513f64-c9fe-4f5c-dbe7-31d036c37e71"},"source":["# memory footprint support libraries/code\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip install gputil\n","!pip install psutil\n","!pip install humanize\n","\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","\n","GPUs = GPU.getGPUs()\n","# XXX: only one GPU on Colab and isn’t guaranteed\n","gpu = GPUs[0]\n","def printm():\n","    process = psutil.Process(os.getpid())\n","    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n","    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","printm()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gputil\n","  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n","Building wheels for collected packages: gputil\n","  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=942140a5fa85a45d380043e712229eee7bacadfd40c3202e2a7c80f3adbf865a\n","  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n","Successfully built gputil\n","Installing collected packages: gputil\n","Successfully installed gputil-1.4.0\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n","Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n","Gen RAM Free: 12.8 GB  |     Proc size: 120.8 MB\n","GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total     11441MB\n"]}]},{"cell_type":"code","metadata":{"id":"bJ6jXiTonXjm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637457002971,"user_tz":300,"elapsed":30012,"user":{"displayName":"Bhanu Chander Reddy Sama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11248088089029933305"}},"outputId":"288e6660-bd89-4b14-b6c7-96141190fbaf"},"source":["import argparse\n","from datetime import datetime\n","import os\n","import time\n","import pandas as pd\n","import numpy as np\n","import shutil\n","import pickle\n","from google.colab import  drive\n","import os\n","import random\n","\n","import matplotlib.pyplot as plt\n","import time\n","\n","from transformers import GPT2Tokenizer,GPT2LMHeadModel,AdamW,get_linear_schedule_with_warmup\n","import torch\n","from torch.nn import CrossEntropyLoss\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler,Dataset\n","from torch.cuda.amp import GradScaler,autocast\n","\n","from tqdm import tqdm\n","from tqdm.notebook import  trange,tnrange\n","\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","stopwords=stopwords.words('english')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","metadata":{"id":"KkeUQer92ASN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637457040710,"user_tz":300,"elapsed":33591,"user":{"displayName":"Bhanu Chander Reddy Sama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11248088089029933305"}},"outputId":"b96c049b-6781-468b-f5e3-e41b1f1af4ca"},"source":["drive.mount('/content/gdrive', force_remount=True)\n","DATA_PATH = 'gdrive/Shared drives/Text_Summarization_Project/'\n","pickles_folder = os.listdir(DATA_PATH)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"mpia5S4s05Yf"},"source":["# Create a Pickle file dump\n","def pickle_dump(use_file_name,data_obj):\n","  local_path = os.path.join(DATA_PATH,use_file_name)\n","  filedump = open(local_path,'wb')\n","  pickle.dump(data_obj,filedump)\n","  filedump.close()\n","  print('Dump object created with filename : ',DATA_PATH+use_file_name)\n","\n","# Create a Pickle file Load\n","def pickle_load(use_file_name):\n","  local_path = os.path.join(DATA_PATH,use_file_name)\n","  fileload = open(local_path,'rb')\n","  data2 = pickle.load(fileload)\n","  fileload.close()\n","  return data2\n","\n","#calculates no of words in newsroom articles\n","def calc_article_sizes(file_name, name):\n","  max_len = 0\n","  article_sizes = {}\n","  print(\"Calculating\",name, \"Article Sizes......\")\n","  data = pickle_load(file_name)\n","  data['text_len'] = data['text'].apply(lambda x: len(x.split()))\n","  max_len = data['text_len'].max()\n","  max_len_indx = data['text_len'].idxmax()\n","  article_sizes = data['text_len'].to_dict()\n","  return max_len, max_len_indx, article_sizes\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXhcPamj05rV","colab":{"base_uri":"https://localhost:8080/","height":469},"executionInfo":{"status":"ok","timestamp":1637457078204,"user_tz":300,"elapsed":15823,"user":{"displayName":"Bhanu Chander Reddy Sama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11248088089029933305"}},"outputId":"c21c266c-0114-4c96-d9c7-0a92e75deb7a"},"source":["start = time.time()\n","# max_len, max_len_filename, article_sizes = calc_article_sizes('processed_data_gpt2.dat', 'newsroom')\n","max_len, max_len_filename, article_sizes = calc_article_sizes('data_processed_for_gpt2.pickle', 'Newsroom')\n","sorted_article_values = np.array(sorted(article_sizes.values()))\n","print(\"saving_article_files_sizes_info...\")\n","\n","pickle_dump('newsroom_article_file_sizes.pickle',article_sizes)\n","# print(len(pd.Series(sorted_article_values).value_counts()))\n","# new_frame = pd.Series(sorted_article_values)\n","# remove_max_vals = \n","#plot the distribution of articles sizes\n","plt.hist(sorted_article_values.tolist(),color='blue',bins=2377, edgecolor = 'black')\n","plt.title(\"Newsroom_Files_Distribution_By_Size(no. of words)\")\n","plt.xlabel('No Of Words')\n","plt.ylabel('Records')\n","plt.show()\n","\n","print('max_length_of_article_in_article: ',max_len, \" and file_index is \", max_len_filename)\n","print(\"total_time_taken\",(time.time()-start), \" seconds\")\n","print(\"mean_length_of_article_articles: \", sum(article_sizes.values())/len(article_sizes))\n","print(\"max_10_lengths_of_article_articles:\", sorted_article_values[-10:])\n","print(\"number_of_articles_greater_than_1000_words: \", sum(sorted_article_values>1000))\n","print(\"number_of_articles_greater_than_1500_words: \", sum(sorted_article_values>1500))\n","print(\"number_of_articles_greater_than_2000_words: \", sum(sorted_article_values>2000))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating Newsroom Article Sizes......\n","saving_article_files_sizes_info...\n","Dump object created with filename :  gdrive/Shared drives/Text_Summarization_Project/newsroom_article_file_sizes.pickle\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["max_length_of_article_in_article:  4960  and file_index is  73242\n","total_time_taken 14.915528059005737  seconds\n","mean_length_of_article_articles:  395.0455076408039\n","max_10_lengths_of_article_articles: [4717 4812 4820 4823 4866 4870 4876 4945 4955 4960]\n","number_of_articles_greater_than_1000_words:  3217\n","number_of_articles_greater_than_1500_words:  1173\n","number_of_articles_greater_than_2000_words:  606\n"]}]},{"cell_type":"code","metadata":{"id":"cZmUHDImYq7d"},"source":["special_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\n","# data_file = pickle_load('processed_data_gpt2.dat')\n","data_file = pickle_load('data_processed_for_gpt2.pickle')\n","del sorted_article_values\n","\n","data_file = data_file.reset_index(drop=True)\n","# data_file['text_total_len'] = data_file['text'].apply(lambda x: len(x.split()))\n","# data_file = data_file[data_file['text_total_len'] <= 5000]\n","# pickle_dump('data_processed_for_gpt2.pickle',data_file)\n","data_file['cummulative_length'] = data_file['text_word_count'] + data_file['sum_word_count'] \n","data_file = data_file[data_file['cummulative_length'] <= 1023]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":573},"id":"MvUuTlM0DJy3","executionInfo":{"status":"ok","timestamp":1637457122436,"user_tz":300,"elapsed":287,"user":{"displayName":"Bhanu Chander Reddy Sama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11248088089029933305"}},"outputId":"66a157f5-3f23-4729-a68c-c60e145204fe"},"source":["data_file.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>text</th>\n","      <th>summary</th>\n","      <th>compression_bin</th>\n","      <th>coverage_bin</th>\n","      <th>density_bin</th>\n","      <th>text_sent_count</th>\n","      <th>sum_sent_count</th>\n","      <th>text_word_count</th>\n","      <th>text_char_count</th>\n","      <th>text_word_density</th>\n","      <th>text_sent_density</th>\n","      <th>sum_word_count</th>\n","      <th>sum_char_count</th>\n","      <th>sum_word_density</th>\n","      <th>sum_sent_density</th>\n","      <th>sent_word_count</th>\n","      <th>sum_sent_word_count</th>\n","      <th>text_total_len</th>\n","      <th>cummulative_length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Watch Closing Arguments : People.com</td>\n","      <td>edt aaron hernandez coming close attorney form...</td>\n","      <td>former new england patriot star face murder ch...</td>\n","      <td>medium</td>\n","      <td>medium</td>\n","      <td>mixed</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>206</td>\n","      <td>1313</td>\n","      <td>0.156773</td>\n","      <td>0.004831</td>\n","      <td>11</td>\n","      <td>59</td>\n","      <td>0.183333</td>\n","      <td>0.083333</td>\n","      <td>206.0</td>\n","      <td>11.0</td>\n","      <td>206</td>\n","      <td>217</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>REGIONS EUROPE - THE GERMAN LOCOMOTIVE LOSES S...</td>\n","      <td>bonn economic summit meeting western leader we...</td>\n","      <td>bonn economic summit meeting western leader we...</td>\n","      <td>low</td>\n","      <td>high</td>\n","      <td>extractive</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>473</td>\n","      <td>3030</td>\n","      <td>0.156054</td>\n","      <td>0.002110</td>\n","      <td>73</td>\n","      <td>468</td>\n","      <td>0.155650</td>\n","      <td>0.013514</td>\n","      <td>473.0</td>\n","      <td>73.0</td>\n","      <td>473</td>\n","      <td>546</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Activate London 2011: David Edelstein</td>\n","      <td>david director grameen foundation technology c...</td>\n","      <td>david director grameen foundation technology c...</td>\n","      <td>low</td>\n","      <td>high</td>\n","      <td>extractive</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>71</td>\n","      <td>499</td>\n","      <td>0.142000</td>\n","      <td>0.013889</td>\n","      <td>53</td>\n","      <td>378</td>\n","      <td>0.139842</td>\n","      <td>0.018519</td>\n","      <td>71.0</td>\n","      <td>53.0</td>\n","      <td>71</td>\n","      <td>124</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>India Becoming a Crucial Cog in the Machine at...</td>\n","      <td>bangalore india june  world biggest computer ...</td>\n","      <td>india provides ibm fastestgrowing market cruci...</td>\n","      <td>high</td>\n","      <td>high</td>\n","      <td>extractive</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>595</td>\n","      <td>3858</td>\n","      <td>0.154185</td>\n","      <td>0.001678</td>\n","      <td>11</td>\n","      <td>73</td>\n","      <td>0.148649</td>\n","      <td>0.083333</td>\n","      <td>595.0</td>\n","      <td>11.0</td>\n","      <td>595</td>\n","      <td>606</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Books Of The Times</td>\n","      <td>christopher lehmannhaupt life imitates world s...</td>\n","      <td>christopher lehmannhaupt life imitates world s...</td>\n","      <td>low</td>\n","      <td>high</td>\n","      <td>extractive</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>504</td>\n","      <td>2915</td>\n","      <td>0.172840</td>\n","      <td>0.001980</td>\n","      <td>63</td>\n","      <td>348</td>\n","      <td>0.180516</td>\n","      <td>0.015625</td>\n","      <td>504.0</td>\n","      <td>63.0</td>\n","      <td>504</td>\n","      <td>567</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               title  ... cummulative_length\n","0               Watch Closing Arguments : People.com  ...                217\n","1  REGIONS EUROPE - THE GERMAN LOCOMOTIVE LOSES S...  ...                546\n","2              Activate London 2011: David Edelstein  ...                124\n","3  India Becoming a Crucial Cog in the Machine at...  ...                606\n","4                                 Books Of The Times  ...                567\n","\n","[5 rows x 20 columns]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"nyoNONGzDQrS","executionInfo":{"status":"ok","timestamp":1637457298631,"user_tz":300,"elapsed":304,"user":{"displayName":"Bhanu Chander Reddy Sama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11248088089029933305"}},"outputId":"238b4cd5-c27e-4225-ef3b-4c56ca1913de"},"source":["data_file['text'][0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'edt aaron hernandez coming close attorney former new england patriot star rested case monday calling three witness closing argument scheduled begin tuesday morning despite short defense presentation trial marathon prosecution called witness stand since trial began jan semiprofessional football player dating sister hernandez fiance shayanna jenkins time murder prosecutor hernandez two men picked lloyd home early morning hour june according testimony drove lloyd secluded industrial park north attleborough massachusetts shot six time prosecutor alleged hernandez orchestrated killing case hernandez without problem prosecutor never presented possible motive killing witness murder weapon never found still prosecution key circumstantial evidence prof hernandez killed lloyd among evidence home surveillance video taken minute shooting show hernandez gripping black item look like gun well joint found near lloyd body dna men it last week prosecution called shayanna jenkins stand testified hernandez told dispose box couple home idea box prosecution contends box contained gun defense claim box may contained marijuana i threw random dumpster jenkins testified adding remember exactly disposed box defense claim prosecution theory made little sense questioning football star kill friend – ruin promising career – apparent reason also accused police conducting shoddy investigation hernandez codefendant ernest wallace carlos ortiz pleaded guilty tried separately jury begin deliberation late tuesday early wednesday'"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"CHnRq5omhnhK"},"source":["train_type = 'extractive'\n","if train_type=='extractive':\n","  pre_train_dataset = data_file[(data_file['density_bin'] == 'extractive') | (data_file['density_bin'] == 'Mixed') ].copy().reset_index(drop=True)\n","elif train_type == 'abstractive':\n","  pre_train_dataset = data_file[(data_file['density_bin'] == 'abstractive') | (data_file['density_bin'] == 'Mixed') ].copy().reset_index(drop=True)\n","else:\n","  pre_train_dataset = data_file[data_file['density_bin']=='Mixed'].copy().reset_index(drop=True)\n","\n","pre_train_dataset = pre_train_dataset[['text','summary']].copy()\n","del data_file\n","# train_text,train_summary = pre_train_dataset['text'],pre_train_dataset['summary']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhCmGyPlChUi","executionInfo":{"status":"ok","timestamp":1637172191378,"user_tz":300,"elapsed":526,"user":{"displayName":"Sumedha Sirikonda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13604935234152853023"}},"outputId":"08b076c7-4ab3-4fa4-e46a-f46befd815ff"},"source":["pickle_dump('updated_pre_train_dt.pickle',pre_train_dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dump object created with filename :  gdrive/Shared drives/Text_Summarization_Project/updated_pre_train_dt.pickle\n"]}]},{"cell_type":"code","metadata":{"id":"WIs3E1H5cIX9"},"source":["def add_special_tokens():\n","\n","\t\"\"\" Returns GPT2 tokenizer after adding separator and padding tokens \"\"\"\n","\ttokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\tspecial_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\n","\tnum_add_toks = tokenizer.add_special_tokens(special_tokens)\n","\treturn tokenizer\n","\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    # if args.n_gpu > 0:\n","    torch.cuda.manual_seed_all(seed)\n"," \n","def word_tokenization_remove_stopwords(x):\n","\t\n","\tcorps = nltk.word_tokenize(x)\n","\tresults = [word.replace('“','').replace('”','').replace(' ’ ','').replace('‘','').replace('—','').replace('…','').replace('•','').replace('¦','').replace('\\x80','').replace('\\x95','') for word in corps if word not in stopwords]\n","\tfinal_txt = \" \".join(results)\n","\treturn final_txt\n","\n","tokenizer = add_special_tokens()\n","# pre_train_dataset['text'] = pre_train_dataset['text'].apply(lambda x: word_tokenization_remove_stopwords(x))\n","# pre_train_dataset['summary'] = pre_train_dataset['summary'].apply(lambda x: word_tokenization_remove_stopwords(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gp-mkKRYnTOc"},"source":["class SummaryDataset(Dataset):\n","\n","  def __init__(self,X,y,index,length=None,mode='train'):\n","    self.tokenizer = add_special_tokens()\n","    self.len = length\n","    self.input = X\n","    self.target = y\n","    self.mode = mode\n","    self.idxs = index \n","\n","  def __len__(self):\n","    return self.len\n","\n","  def __getitem__(self,idx):\n","\n","    if self.mode=='valid':\n","      idx = self.idxs[-idx]\n","    elif self.mode=='test':\n","      idx = self.idxs[-idx-self.len]   # assuming valid and test set of same sizes\n","    else:\n","      idx = self.idxs[idx]\n","    \n","    text = self.tokenizer.encode(self.tokenizer.pad_token)*1024\n","    content = self.input[idx] + self.tokenizer.sep_token + self.target[idx]\n","\n","    token_vecs = self.tokenizer.encode(content)\n","    \n","    if len(token_vecs) > 1024:\n","      token_vecs = token_vecs[:1024]\n","    \n","    text[:len(token_vecs)] = token_vecs\n","    final_val = torch.tensor(np.array(text))\n","    # print('Tensor shape: ',final_val.shape,' length of token vecs : ',len(self.input[idx].split()))\n","    # print('Final tensor : ',final_val)\n","    sample = {'article': final_val, 'sum_idx': len(self.input[idx].split())}\n","    return sample"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PSho3EeYqagX"},"source":["def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n","    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n","        Args:\n","            logits: logits distribution shape (vocabulary size)\n","            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n","            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n","                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n","        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n","    \"\"\"\n","\n","    # print('Logits dimension : ',logits.dim())\n","    # assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n","    top_k = min(top_k, logits.size(-1))  # Safety check\n","    print('Logits size : ',logits.size(-1),end='\\n\\n')\n","    # print('Top-K : ',top_k,end='\\n\\n')\n","    if top_k > 0:\n","      print('In the TOP-K Loop with top_k value : ',top_k )\n","      print('Logits values : ',logits)\n","      print('Top K for cont',torch.topk(logits, top_k))\n","      \n","      # Remove all tokens with a probability less than the last token of the top-k\n","      indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n","      print('Size of Indices to remove : ',(indices_to_remove.shape))\n","      print('Indices to be removed from top_k filtering: ',indices_to_remove)\n","      logits[indices_to_remove] = filter_value\n","\n","    if top_p > 0.0:\n","      # print('In the Top P loop with top_p value :', top_p)\n","      sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","      # sorted_logits = logits\n","      # print('Sorted Logits are: ',sorted_logits)\n","      # print('softmax result = ',F.softmax(sorted_logits))\n","      cumulative_probs = torch.cumsum(F.softmax(sorted_logits,dim=-1),dim=-1)\n","      # print('Cummulative Probability : ',cumulative_probs)\n","      # Remove tokens with cumulative probability above the threshold\n","      sorted_indices_to_remove = cumulative_probs > top_p\n","      # Shift the indices to the right to keep also the first token above the threshold\n","      sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n","      sorted_indices_to_remove[..., 0] = 0\n","\n","      indices_to_remove = sorted_indices[sorted_indices_to_remove]\n","      # print('Indices to remove from top_p filtering: ',indices_to_remove)\n","      logits[indices_to_remove] = filter_value\n","    # print('Logits type : ',type(logits))\n","    # print('Logits shape : ',logits.shape)\n","    # print('Logits indices :',logits)\n","    return logits\n","\n","def sample_seq(model, context, length, device, temperature=1, top_k=0, top_p=0.0):\n","\n","  context = torch.tensor(context, dtype=torch.long, device=device)\n","  context = context.unsqueeze(0)\n","  generated = context\n","  # generated = torch.nan_to_num(context,nan=2.0,posinf=1.0)\n","  \n","  # print('Generated Context: ',generated)\n","  print('Length of Sequence: ',length)\n","  set_seed(42)\n","  with torch.no_grad():\n","    for _ in tnrange(length):\n","\n","      inputs = {'input_ids': generated}\n","      print(inputs['input_ids'])\n","      device_inps = inputs['input_ids'].to(device)\n","      outputs = model(inputs)\n","      # print('Outputs generated: ',outputs)\n","      # outputs = model.generate(device_inps,max_length=1,top_k=0) #,top_p=0.5,temperature=0.8       \n","      next_token = torch.multinomial(F.softmax(outputs, dim=-1,dtype=torch.float), num_samples=1)\n","      print('Generated Next Token : ',next_token)\n","      generated = torch.cat((generated, next_token), dim=1) #.unsqueeze(0)\n","      \n","  return generated\n","\n"," \n","def generate_sample(data, tokenizer, model, num=1, eval_step=False, length=100, temperature=0.7, top_k=100, top_p=0.5, device=torch.device('cuda')):\n","  \"\"\" Generate summaries for \"num\" number of articles.\n","        Args:\n","            data = GPT21024Dataset object\n","            tokenizer = gpt/gpt2 tokenizer\n","            model = gpt/gpt2 model\n","            num = number of articles for which summaries has to be generated\n","            eval_step = can be True/False, checks generating during evaluation or not\n","    \"\"\"     \n","  # print('Value of Num: ', num)\n","  for i in range(num):\n","    sample = data[i]\n","    idx = sample['sum_idx']\n","    context = sample['article'][:idx].tolist()\n","    # print('Validation Context : ',context)\n","    summary = sample['article'][idx+1:][:100].tolist()\n","    # print('Validation Summary : ',summary)\n","    generated_text = sample_seq(model, context, length, device, temperature, top_k, top_p)\n","\n","    generated_text = generated_text[0, len(context):].tolist()\n","    print('Generated Summary text : ',generated_text)\n","    text = tokenizer.convert_ids_to_tokens(generated_text,skip_special_tokens=True)\n","    text = tokenizer.convert_tokens_to_string(text)\n","    if eval_step==False:\n","        print('new_article', end='\\n\\n')\n","        print(tokenizer.decode(context), end='\\n\\n')\n","        print(\"generated_summary\", end='\\n\\n')\n","        print(text, end='\\n\\n')\n","        print('actual_summary', end='\\n\\n')\n","        print(tokenizer.decode(summary), end='\\n\\n')\n","    else:\n","        print(tokenizer.decode(context), end='\\n\\n')\n","        print(\"generated_summary\", end='\\n\\n')\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y8T9eYxpFZiq"},"source":["num_train_epochs =  5\n","batch_size=1\n","gradient_accumulation_steps= 32\n","learning_rate = 5e-5\n","maximum_gradient_normalization = 1\n","scaler = GradScaler()\n","\n","def train(model,tokenizer,train_dataset,valid_dataset,indexes):\n","  \n","  training_sample = RandomSampler(train_dataset)\n","  train_dl = DataLoader(train_dataset,sampler=training_sample,batch_size=batch_size)\n","  loss_function = CrossEntropyLoss(ignore_index=indexes)\n","  optimizer = AdamW(model.parameters(),lr=learning_rate)\n","  # scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,warmup_steps=200,t_total= 80000,last_epoch=-1)\n","  scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,num_warmup_steps=200,num_training_steps= 80000)\n","  \n","  global_step,tr_loss,log_loss = 0,0.0,0.0\n","  model.zero_grad()\n","  train_iterator = trange(int(num_train_epochs),desc='Epoch')\n","  set_seed(42)\n","  \n","  for _ in train_iterator:\n","    epoch_iterator = tqdm(train_dl, desc=\"Training\")\n","    for step, batch in enumerate(epoch_iterator):\n","      X_train,y_train = batch['article'].clone().detach(),batch['article'].clone().detach()\n","      \n","      X_train = X_train.to(device=device)\n","      y_train = y_train.to(device=device)\n","      model.train()\n","      with autocast():\n","        y_train_predicted = model(X_train)[0]\n","        # print('Predicted Train:',y_train_predicted)\n","        idx = batch['sum_idx'].item()\n","        shift_logits = y_train_predicted[..., batch['sum_idx']:-1, :].contiguous()\n","        shift_labels = y_train[..., batch['sum_idx']+1:].contiguous()\n","        train_losses = loss_function(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","\n","      train_losses = train_losses/gradient_accumulation_steps\n","      scaler.scale(train_losses).backward()\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), maximum_gradient_normalization)\n","      tr_loss += train_losses.item()\n","\n","      print('Training Loss : ',train_losses)\n","      if (step + 1) % gradient_accumulation_steps == 0:\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        model.zero_grad()\n","        global_step += 1\n","        logging_loss = tr_loss\n","        print(\"loss:\", train_losses.item(), end='\\n\\n')\n","        if (step + 1)/gradient_accumulation_steps == 1.0:\n","          print('After 1st update: ', end='\\n\\n')\n","          generate_sample(valid_dataset, tokenizer,model, num=2, eval_step=False,device=device) #,temperature=0.7,top_k=200,top_p=0.5\n","                \n","                \n","      if (step + 1) % (10*gradient_accumulation_steps) == 0:\n","        results = evaluate(model, valid_dataset, indexes, global_step)\n","        print('After', global_step+1,'updates: ', end='\\n\\n')\n","        generate_sample(valid_dataset, tokenizer,model, num=2, eval_step=True,device=device)\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UWouJBRHhFIN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637027441300,"user_tz":300,"elapsed":4957,"user":{"displayName":"Sumedha Sirikonda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13604935234152853023"}},"outputId":"7b660de3-6ec2-4f81-f3e1-df17e9b21b4b"},"source":["if torch.cuda.is_available():\n","  device = torch.device('cuda:0')\n","else:\n","  device = torch.device('cpu')\n","\n","print('Device Used: ',device)\n","printm()\n","pre_train_dataset = pickle_load('updated_pre_train_dt.pickle')\n","training_set = int(pre_train_dataset.shape[0]*0.7)\n","validation_set = int(pre_train_dataset.shape[0]*0.3)\n","index = pre_train_dataset.index \n","train_dataset = SummaryDataset(pre_train_dataset['text'],pre_train_dataset['summary'],index,training_set,'train')\n","valid_dataset = SummaryDataset(pre_train_dataset['text'],pre_train_dataset['summary'],index,validation_set,'valid')\n","# print('Validation dataset size : ',valid_dataset)\n","ignore_idx = tokenizer.pad_token_id\n","#  read batches of RAM and not on Disk space.\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","model.resize_token_embeddings(len(tokenizer))\n","model.to(device)\n","print('After Model loading')\n","printm()\n","# del pre_train_dataset\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device Used:  cuda:0\n","Gen RAM Free: 24.3 GB  |     Proc size: 6.1 GB\n","GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total     16280MB\n","After Model loading\n","Gen RAM Free: 24.3 GB  |     Proc size: 6.4 GB\n","GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total     16280MB\n"]}]},{"cell_type":"code","metadata":{"id":"oGNTMvrOvNoo"},"source":["train(model,tokenizer,train_dataset,valid_dataset,ignore_idx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"id":"rreDmTmOcHO-","executionInfo":{"status":"ok","timestamp":1637027452920,"user_tz":300,"elapsed":142,"user":{"displayName":"Sumedha Sirikonda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13604935234152853023"}},"outputId":"f9646b3e-9848-4e5d-e7e3-447b8abd6a4a"},"source":["text = pre_train_dataset['text'][0]\n","inp_ids = tokenizer.encode(text)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'bonn economic summit meeting western leader west germany staunchly resisted call use head economic steam like locomotive tug europe lagging economy recession oil crisis induced end however germany politician industrialist flattered anyone asked play locomotive cost oil still climbing europe economy locked another downturn west germany run steam leader becoming increasingly pessimistic people remain convinced germany still one line heavier piece rolling stock also came realize immune economic difficulty plague industrialized nation sure began well west germany fear spiraling oil price led factory owner stockpile industry booked heavy order assembly line hummed growth first quarter acceptable percent second quarter however pace slackened consumer spending eased energy cost rose order overseas dried developing country spent money oil rather turnkey plant west germany production slowed ripple united state recession begun earlier year interest rate screwed ever higher fight inflation spread across atlantic half picture traditionally germany juggler current account using strong export earnings offset capital german company invest abroad money german tourist spend overseas considerable sum foreign worker send back family place like turkey yugoslavia combination slackened export sharply increased payment import among thing costly oil enormous outlay german tourist abroad huge overseas industrial investment swelled deficit billion billion deficit reduced current account shamble current account showed surplus nearly billion expected billion deficit year germany economic engineer central bank bundesbank best attract capital back germany raising key lending rate record height last spring government went borrowing saudi arabia close gap measure partial success others brought unwanted side effect high interest rate lost much attraction came time united state fight doubledigit inflation also lifted interest rate alltime high addition made money expensive dampening economy time many cheap money needed stoke fire high united state interest rate attracted investor german mark dollar caused mark drop value percent compared dollar last year year end mark relatively low value making import costly lifting inflationary spiral germany little stem decline export caused worldwide recession year ended outlook offered little consolation october bonn minister economics otto lambsdorff predicted economic growth zero percent shortly thereafter gloomy joint report germany five leding economic institute expected country real gross national product stand still percent growth beginning new year even somber prediction considered optimistic address nation chancellor helmut schmidt warned hard piece road lying ahead u remark likely prompted prediction economist saying economic activity shrink much percent year survey economist commerzbank country thirdlargest commercial bank consumer buying likely stagnate tax cut promised government early year offset rising social security payment government pumppriming question survey said unemployment go percent important number people shortened hour jump bank economist said speech mr schmidt rejected belittlingly termed rhetoric belttightening junior minister government andreas von bulow minister research told reporter later government consider limiting travel currency expenditure abroad high deficit prof chronic ailment immediately called order senior colleague meantime plane resort tenerife remain booked john tagliabue'"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"q6KOSpgrcHQ5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_I46pb0cHTZ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CcXRU4tFcHVH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0tr_VHvCqJdi"},"source":["def evaluate(args, model, eval_dataset, ignore_index, global_step=None):\n","    \"\"\" Returns perplexity score on validation dataset.\n","        Args:\n","            args: dict that contains all the necessary information passed by user while training\n","            model: finetuned gpt/gpt2 model\n","            eval_dataset: GPT21024Dataset object for validation data\n","            global_step: no. of times gradients have backpropagated\n","            ignore_index: token not considered in loss calculation\n","    \"\"\"\n","    results = {}\n","    eval_sampler = SequentialSampler(eval_dataset)\n","    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=batch_size)\n","    loss_fct = CrossEntropyLoss(ignore_index=ignore_idx) #ignores padding token for loss calculation\n","\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    model.eval()\n","    for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n","        inputs, labels = batch['article'].to(device), batch['article'].to(device)\n","        \n","        with torch.no_grad():\n","            logits = model(inputs)[0]\n","            idx = batch['sum_idx'].item() # index of separator token\n","            # only consider loss on reference summary just like seq2seq models\n","            shift_logits = logits[..., batch['sum_idx']:-1, :].contiguous()\n","            shift_labels = labels[..., batch['sum_idx']+1:].contiguous()\n","            lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","            eval_loss += lm_loss.mean().item()\n","        nb_eval_steps += 1\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    perplexity = torch.exp(torch.tensor(eval_loss))\n","\n","    result = {\n","        \"perplexity\": perplexity\n","    }\n","    print(\"perplexity:\", perplexity.item())\n","\n","    if global_step:\n","        output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n","        with open(output_eval_file, \"a\") as f:\n","            for key in sorted(result.keys()):\n","                f.write('\\n\\n')\n","                f.write(\"time = %s, %s = %s, step = %s\\n\" % (datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), key, str(result[key]), str(global_step)))\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"riEcEP9HOCUs","executionInfo":{"status":"ok","timestamp":1637023776043,"user_tz":300,"elapsed":201,"user":{"displayName":"Sumedha Sirikonda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13604935234152853023"}},"outputId":"69aeab8e-d711-4e2d-be4f-f22d2535f3d9"},"source":["tokenizer.decode(187)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'�'"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"fVyiByuhUIcK"},"source":["logits_val = torch.tensor([[ 4189,    77,  3034, 14237,  3249,  8830,  3554,  7421,   308,  2224,\n","            88, 34700,   306, 26643,   869,   779,  1182,  3034, 13324,   588,\n","         33334, 19138, 27762, 11063,   431,   300, 16406,  3773, 16457,  3056,\n","          4902, 18268,   886,  2158,   308,  2224,    88, 14971,  7593,   396,\n","          6228,  4400,  2687,  1965,   711, 33334, 19138,  1575,  3056,   991,\n","         14281, 11063,   431,  3773,  8970,  1194, 34540,  7421,   308,  2224,\n","            88,  1057, 13324,  3554,  5033,  6481, 46392,   661,  3520,  9431,\n","           308,  2224,    88,   991,   530,  1627, 20140,  3704, 10708,  4283,\n","           635,  1625,  6537, 10900,  3034,  8722, 23684, 47670,  3277,  1654,\n","          2540,   880,  7421,   308,  2224,    88,  3252,  9158,  4272,  3056,\n","          2756,  2957,  8860,  4870, 45360,  2831, 21765,  4334,  1502, 10474,\n","          1627,  1311,  1150,  3349,   717,  3860, 10909,  1411,  1218,  3860,\n","          2158,  8761, 30740,  2945,  7172,  4581, 44828,  2568,  1575,  8278,\n","          1502, 11292, 16577,  5922,  1499,  3377,  1637,  3056,  2138,  1210,\n","          2539,  4618,  7421,   308,  2224,    88,  3227, 20955, 42462, 16503,\n","          1181, 16457,  9258,  2961,   614,  1393,  2494, 27527,  1683,  2440,\n","          1907, 10610,  4104,  1973,   379,    75,  5109,  2063,  4286, 16083,\n","           308,  2224,    88, 24873,  1754,  1459,  1848,  1262,  1913, 10784,\n","         12042, 11677,  3139,   308,  2224,  1664,  1325, 10522,  1637,   308,\n","          2224, 18473,  4341, 11292, 11091,  2160,  3215,  8383,  3758,   736,\n","          1641,  1295,   588, 26876,   331,  1018, 26388,   544,  6087, 30740,\n","          2945, 10784, 18939,  3220,  6074,  1330,  1871,  1517, 16378,  3056,\n","          9812,   503, 10724,   308,  2224, 18473, 10522,  3236, 11292,  7593,\n","          4896,  1509, 11978, 11807,  2997,  2997, 11807,  5322,  1459,  1848,\n","         23565,   903,  1459,  1848,  3751, 18201,  3016,  2997,  2938,  2997,\n","         11807,   614,   308,  2224,    88,  3034, 11949,  4318,  3331, 12207,\n","           274, 17796,  1266,  4729,  3139,   736,   308,  2224,    88,  8620,\n","          1994, 20983,  2494,  1700,  6001,   938,  6076,  1230,  1816, 23669,\n","           473, 47928,   610,   397,   544,  1969,  7625,  3953, 13027,  1943,\n","          1854,  3181, 19125,  1735,  1245,  1029,  1393,  2494,  2626,   881,\n","         17416,  1625,   640, 16503,  1181,  1907, 15229,   328,   270, 10610,\n","           635, 13663,  1393,  2494,   477,  2435,  1029,  3090,   925,  1637,\n","          5789, 21151,  3101,  3773,   640,   867,  7026,  1637,  2622,   336,\n","          2088,  2046,  1029, 16503,  1181,  1393,  2494, 12725, 15811,   308,\n","          2224,  1317,  8872,  4073,  1317,  4268,  1988,  1411,  3688,  8872,\n","           938,   614,   614,   886,  1317,  5365,  1877,  1988,  1642,  1330,\n","         16378, 16842, 10610,   560, 23642,   308,  2224,    88,  1310, 10717,\n","          7794, 10784,  4073,  8688, 16457,   614,  4444, 19360,  4438,  1310,\n","         45194, 19318,  2023,  5351,    77,  5342, 12446,   267, 33955, 30592,\n","          1443, 40180,   487, 11001,  3034,  3349,  6632,  1411,  8972, 19547,\n","         46400,  6466,   989,   308,  2224,    88,  1936,  2957,   278,  3034,\n","         24224,  2938,  1499,  1103, 10319,  2260,  1720,  1302,   991,  1411,\n","          3349,  3726,   649,   614,   772,  3870,   527, 17724,  3177, 16915,\n","          2209,  3277, 30006, 18030,   315,  5513, 21184,  7728,  1327,  3704,\n","          2975,  9105,  4058,   334,  6919,  1884, 12053, 17724, 17646,  2282,\n","          3034,  3842, 22085,   881,  1411,   614,  5526, 17646,  4412, 14969,\n","           962,  1499,  2368, 28209,  5068,  3331,  7172,  7067,  1884,     0]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gjc5sZ46c-Jr"},"source":["logits_val[0, 349:].tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":215},"id":"iaXs63BpbqTE","executionInfo":{"status":"ok","timestamp":1637028636490,"user_tz":300,"elapsed":2129,"user":{"displayName":"Sumedha Sirikonda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13604935234152853023"}},"outputId":"7a06e12b-ff7d-4214-e301-dff2c3d381b8"},"source":["txt = pre_train_dataset['text'][0]\n","print(txt,end='\\n\\n')\n","input_ids = tokenizer.encode(txt,return_tensors='pt')\n","model.to('cpu')\n","outputs = model.generate(input_ids=input_ids,max_length=50)\n","prediction = []\n","for element in outputs:\n","  prediction.append(tokenizer.decode(element,skip_special_tokens=True))\n","' '.join(prediction)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Input length of input_ids is 564, but ``max_length`` is set to 50. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"]},{"output_type":"stream","name":"stdout","text":["bonn economic summit meeting western leader west germany staunchly resisted call use head economic steam like locomotive tug europe lagging economy recession oil crisis induced end however germany politician industrialist flattered anyone asked play locomotive cost oil still climbing europe economy locked another downturn west germany run steam leader becoming increasingly pessimistic people remain convinced germany still one line heavier piece rolling stock also came realize immune economic difficulty plague industrialized nation sure began well west germany fear spiraling oil price led factory owner stockpile industry booked heavy order assembly line hummed growth first quarter acceptable percent second quarter however pace slackened consumer spending eased energy cost rose order overseas dried developing country spent money oil rather turnkey plant west germany production slowed ripple united state recession begun earlier year interest rate screwed ever higher fight inflation spread across atlantic half picture traditionally germany juggler current account using strong export earnings offset capital german company invest abroad money german tourist spend overseas considerable sum foreign worker send back family place like turkey yugoslavia combination slackened export sharply increased payment import among thing costly oil enormous outlay german tourist abroad huge overseas industrial investment swelled deficit billion billion deficit reduced current account shamble current account showed surplus nearly billion expected billion deficit year germany economic engineer central bank bundesbank best attract capital back germany raising key lending rate record height last spring government went borrowing saudi arabia close gap measure partial success others brought unwanted side effect high interest rate lost much attraction came time united state fight doubledigit inflation also lifted interest rate alltime high addition made money expensive dampening economy time many cheap money needed stoke fire high united state interest rate attracted investor german mark dollar caused mark drop value percent compared dollar last year year end mark relatively low value making import costly lifting inflationary spiral germany little stem decline export caused worldwide recession year ended outlook offered little consolation october bonn minister economics otto lambsdorff predicted economic growth zero percent shortly thereafter gloomy joint report germany five leding economic institute expected country real gross national product stand still percent growth beginning new year even somber prediction considered optimistic address nation chancellor helmut schmidt warned hard piece road lying ahead u remark likely prompted prediction economist saying economic activity shrink much percent year survey economist commerzbank country thirdlargest commercial bank consumer buying likely stagnate tax cut promised government early year offset rising social security payment government pumppriming question survey said unemployment go percent important number people shortened hour jump bank economist said speech mr schmidt rejected belittlingly termed rhetoric belttightening junior minister government andreas von bulow minister research told reporter later government consider limiting travel currency expenditure abroad high deficit prof chronic ailment immediately called order senior colleague meantime plane resort tenerife remain booked john tagliabue\n","\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'bonn economic summit meeting western leader west germany staunchly resisted call use head economic steam like locomotive tug europe lagging economy recession oil crisis induced end however germany politician industrialist flattered anyone asked play locomotive cost oil still climbing europe economy locked another downturn west germany run steam leader becoming increasingly pessimistic people remain convinced germany still one line heavier piece rolling stock also came realize immune economic difficulty plague industrialized nation sure began well west germany fear spiraling oil price led factory owner stockpile industry booked heavy order assembly line hummed growth first quarter acceptable percent second quarter however pace slackened consumer spending eased energy cost rose order overseas dried developing country spent money oil rather turnkey plant west germany production slowed ripple united state recession begun earlier year interest rate screwed ever higher fight inflation spread across atlantic half picture traditionally germany juggler current account using strong export earnings offset capital german company invest abroad money german tourist spend overseas considerable sum foreign worker send back family place like turkey yugoslavia combination slackened export sharply increased payment import among thing costly oil enormous outlay german tourist abroad huge overseas industrial investment swelled deficit billion billion deficit reduced current account shamble current account showed surplus nearly billion expected billion deficit year germany economic engineer central bank bundesbank best attract capital back germany raising key lending rate record height last spring government went borrowing saudi arabia close gap measure partial success others brought unwanted side effect high interest rate lost much attraction came time united state fight doubledigit inflation also lifted interest rate alltime high addition made money expensive dampening economy time many cheap money needed stoke fire high united state interest rate attracted investor german mark dollar caused mark drop value percent compared dollar last year year end mark relatively low value making import costly lifting inflationary spiral germany little stem decline export caused worldwide recession year ended outlook offered little consolation october bonn minister economics otto lambsdorff predicted economic growth zero percent shortly thereafter gloomy joint report germany five leding economic institute expected country real gross national product stand still percent growth beginning new year even somber prediction considered optimistic address nation chancellor helmut schmidt warned hard piece road lying ahead u remark likely prompted prediction economist saying economic activity shrink much percent year survey economist commerzbank country thirdlargest commercial bank consumer buying likely stagnate tax cut promised government early year offset rising social security payment government pumppriming question survey said unemployment go percent important number people shortened hour jump bank economist said speech mr schmidt rejected belittlingly termed rhetoric belttightening junior minister government andreas von bulow minister research told reporter later government consider limiting travel currency expenditure abroad high deficit prof chronic ailment immediately called order senior colleague meantime plane resort tenerife remain booked john tagliabue'"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"7exCY9lnzaXU"},"source":["# torch.multinomial(F.softmax(logits_val, dim=-1,dtype=torch.float), num_samples=1)\n","logits_val = logits_val.t()\n","next_token = torch.multinomial(F.softmax(logits_val, dim=-1,dtype=torch.float),num_samples=1)\n","# generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n","next_token"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5yBgeSNY3wKF"},"source":["next_token.t().unsqueeze(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wMOIbo18UItG"},"source":["F.softmax(logits_val,dtype=torch.float,dim=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bTKOnPCDcT9X"},"source":["total_t0 = time.time()\n","\n","training_stats = []\n","\n","model = model.to(device)\n","\n","\n","num_train_epochs =  3\n","batch_size=1\n","gradient_accumulation_steps= 64\n","learning_rate = 5e-3\n","maximum_gradient_normalization = 1\n","scaler = GradScaler()\n","sample_every = 100\n","\n","def train(model,tokenizer,train_dataset,valid_dataset,indexes):\n","  \n","  training_sample = RandomSampler(train_dataset)\n","  train_dl = DataLoader(train_dataset,sampler=training_sample,batch_size=batch_size)\n","  valid_dl = DataLoader(valid_dataset)\n","  loss_function = CrossEntropyLoss(ignore_index=indexes)\n","  optimizer = AdamW(model.parameters(),lr=learning_rate)\n","  # scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,warmup_steps=200,t_total= 80000,last_epoch=-1)\n","  scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,num_warmup_steps=200,num_training_steps= 80000,last_epoch=-1)\n","  \n","\n","  for epoch_i in range(0, num_train_epochs):\n","\n","      # ========================================\n","      #               Training\n","      # ========================================\n","\n","      print(\"\")\n","      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","      print('Training...')\n","\n","      t0 = time.time()\n","\n","      total_train_loss = 0\n","\n","      model.train()\n","\n","      for step, batch in enumerate(train_dl):\n","\n","          X_train,y_train = batch['article'].clone().detach(),batch['article'].clone().detach()\n","      \n","          X_train = X_train.to(dtype=torch.long, device=device)\n","          y_train = y_train.to(dtype=torch.long, device=device)\n","          \n","          model.zero_grad()        \n","\n","          outputs = model(  X_train,\n","                            labels=y_train, \n","                            # attention_mask = b_masks,\n","                            token_type_ids=None\n","                          )\n","\n","          loss = outputs[0]  \n","\n","          batch_loss = loss.item()\n","          total_train_loss += batch_loss\n","\n","          # Get sample every x batches.\n","          if step % sample_every == 0 and not step == 0:\n","\n","              elapsed = format_time(time.time() - t0)\n","              print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n","\n","              model.eval()\n","\n","              sample_outputs = model.generate(\n","                                      bos_token_id=random.randint(1,30000),\n","                                      do_sample=True,   \n","                                      top_k=50, \n","                                      max_length = 200,\n","                                      top_p=0.95, \n","                                      num_return_sequences=1\n","                                  )\n","              for i, sample_output in enumerate(sample_outputs):\n","                    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n","              \n","              model.train()\n","\n","          loss.backward()\n","\n","          optimizer.step()\n","\n","          scheduler.step()\n","\n","      # Calculate the average loss over all of the batches.\n","      avg_train_loss = total_train_loss / len(train_dataloader)       \n","      \n","      # Measure how long this epoch took.\n","      training_time = format_time(time.time() - t0)\n","\n","      print(\"\")\n","      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","      print(\"  Training epoch took: {:}\".format(training_time))\n","          \n","      # ========================================\n","      #               Validation\n","      # ========================================\n","\n","      print(\"\")\n","      print(\"Running Validation...\")\n","\n","      t0 = time.time()\n","\n","      model.eval()\n","\n","      total_eval_loss = 0\n","      nb_eval_steps = 0\n","\n","      # Evaluate data for one epoch\n","      for batch in validation_dataloader:\n","          \n","          b_input_ids = batch[0].to(device)\n","          b_labels = batch[0].to(device)\n","          b_masks = batch[1].to(device)\n","          \n","          with torch.no_grad():        \n","\n","              outputs  = model(b_input_ids, \n","  #                            token_type_ids=None, \n","                              attention_mask = b_masks,\n","                              labels=b_labels)\n","            \n","              loss = outputs[0]  \n","              \n","          batch_loss = loss.item()\n","          total_eval_loss += batch_loss        \n","\n","      avg_val_loss = total_eval_loss / len(validation_dataloader)\n","      \n","      validation_time = format_time(time.time() - t0)    \n","\n","      print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","      print(\"  Validation took: {:}\".format(validation_time))\n","\n","      # Record all statistics from this epoch.\n","      training_stats.append(\n","          {\n","              'epoch': epoch_i + 1,\n","              'Training Loss': avg_train_loss,\n","              'Valid. Loss': avg_val_loss,\n","              'Training Time': training_time,\n","              'Validation Time': validation_time\n","          }\n","      )\n","\n","  print(\"\")\n","  print(\"Training complete!\")\n","  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tTRUV7QlUIu3","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1637024306925,"user_tz":300,"elapsed":180,"user":{"displayName":"Sumedha Sirikonda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13604935234152853023"}},"outputId":"de155ac6-9c30-4c7a-d7f6-53e9601e59d9"},"source":["tokenizer.decode(281)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' an'"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"ihHbcdG7UIyN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kG06aabOUIz1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cPow91BHeL1u"},"source":[""],"execution_count":null,"outputs":[]}]}